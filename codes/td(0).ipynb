{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b04d877-e4c3-4325-b2af-e04551a00e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4add868d-fa44-4009-8d8e-11691827220f",
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_pots={\n",
    "    (0,3): 10,\n",
    "    (1,1): \"-1\",\n",
    "    (3,0): 1,\n",
    "    (3,3): \"20\",\n",
    "    (0,2): \"-20\",\n",
    "    (2,0): \"-2\",\n",
    "    (2,3): -45\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a39f8b89-af49-486a-90c9-504a01d255c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.,   0., -20.,  10.],\n",
       "       [  0.,  -1.,   0.,   0.],\n",
       "       [ -2.,   0.,   0., -45.],\n",
       "       [  1.,   0.,   0.,  20.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "grid_size=4\n",
    "rewards=np.zeros((grid_size,grid_size))\n",
    "for pos,reward in reward_pots.items():\n",
    "    rewards[pos]=reward\n",
    "rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77e8c2de-f16f-499c-97fe-0643bddd526a",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "# Define the deterministic transition dynamics\n",
    "def move(position, action):\n",
    "    x, y = position\n",
    "    if action == \"3\" and x > 0:\n",
    "        return (x - 1, y)\n",
    "    elif action == \"1\" and x < grid_size - 1:\n",
    "        return (x + 1, y)\n",
    "    elif action == \"0\" and y > 0:\n",
    "        return (x, y - 1)\n",
    "    elif action == \"2\" and y < grid_size - 1:\n",
    "        return (x, y + 1)\n",
    "    else:\n",
    "        return position # If the move is not possible, stay in the same position\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32d8f06d-cab1-4d22-a90f-c42f6ec4fbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Value iteration parameters\n",
    "gamma = 0.7  # Discount factor\n",
    "theta = 0.00001  # Threshold for convergence\n",
    "actions = [\"0\", \"1\", \"2\", \"3\"]\n",
    "num_episodes=100000\n",
    "value_func=np.zeros((4,4))\n",
    "count_func=np.zeros((4,4))\n",
    "\n",
    "# Initialize the number of episodes\n",
    "for _ in range(num_episodes):\n",
    "    # Start at the initial state (0, 0)\n",
    "    state = (0, 0)\n",
    "    # Initialize an empty list to store the episode\n",
    "    episode = []\n",
    "    \n",
    "    # Generate an episode\n",
    "    while True:\n",
    "        # Select a random action from the available actions\n",
    "        action = random.choice(actions)\n",
    "        # Determine the next state based on the current state and action\n",
    "        next_state = move(state, action)\n",
    "        # Get the reward for the next state\n",
    "        reward = rewards[next_state]\n",
    "        # Append the (state, action, reward) tuple to the episode list\n",
    "        episode.append((state, action, reward,next_state))\n",
    "        # Update the current state to the next state\n",
    "        state = next_state\n",
    "        # If the terminal state (3, 3) is reached, end the episode\n",
    "        if state == (3, 3):\n",
    "            break\n",
    "    \n",
    "    # Initialize the return value G to 0\n",
    "    G = 0\n",
    "    # Iterate over the episode in reverse order\n",
    "    for t in reversed(range(len(episode))):\n",
    "        # Extract the state, action, and reward at time t\n",
    "        state, action, reward,next_state = episode[t]\n",
    "        # Update the return value G\n",
    "        \n",
    "        row, col = state\n",
    "        \n",
    "        # Incrementally update V(s)\n",
    "        # Increase the visit count for the current state\n",
    "        count_func[row, col] += 1\n",
    "        # Calculate the learning rate alpha\n",
    "        alpha = 1.0 / count_func[row, col]\n",
    "        # Update the value function for the current state\n",
    "        value_func[row, col] += alpha * (reward+gamma*(value_func[next_state] - value_func[row, col])\n",
    "\n",
    "# Print the rewards, value function, and visit count\n",
    "print(rewards, value_func, count_func, sep='\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95316bd4-cdbe-4fb8-ac77-13edaa1fc391",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
