{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a70758a0-a7c5-4df1-9b79-9736504d1e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random as rd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "0496fed7-cd1a-4282-afc7-ab5edd700e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "grid_size=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "17119234-9bc1-4cad-ba4c-e9043b580032",
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_pots={\n",
    "    (0,3): 10,\n",
    "    (1,1): \"-1\",\n",
    "    (3,0): 1,\n",
    "    (3,2): \"-20\",\n",
    "    (2,3): -45\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "2923ed24-4e59-4af7-9614-fa9b5a29a7ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.,   0.,   0.,  10.],\n",
       "       [  0.,  -1.,   0.,   0.],\n",
       "       [  0.,   0.,   0., -45.],\n",
       "       [  1.,   0., -20.,   0.]])"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards=np.zeros((grid_size,grid_size))\n",
    "for pos,reward in reward_pots.items():\n",
    "    rewards[pos]=reward\n",
    "rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "bf650c54-8db2-46f2-a0f9-82d039ae94a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reward(position):\n",
    "    return rewards[position]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "f221fe3d-f15b-4260-82f7-598d4d5a4eb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.0"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_reward((0,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "9d1540c2-db78-400a-bb78-99ead8bf2a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the deterministic transition dynamics\n",
    "def move(position, action):\n",
    "    x, y = position\n",
    "    if action == \"up\" and x > 0:\n",
    "        return (x - 1, y)\n",
    "    elif action == \"down\" and x < grid_size - 1:\n",
    "        return (x + 1, y)\n",
    "    elif action == \"left\" and y > 0:\n",
    "        return (x, y - 1)\n",
    "    elif action == \"right\" and y < grid_size - 1:\n",
    "        return (x, y + 1)\n",
    "    else:\n",
    "        return position  # If the move is not possible, stay in the same position\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "413d5459-fc78-4ff9-8cd1-886996a752bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "value_func=np.zeros((4,4))\n",
    "\n",
    "# Value iteration parameters\n",
    "gamma = 0.7  # Discount factor\n",
    "theta = 0.00001  # Threshold for convergence\n",
    "actions = [\"up\", \"down\", \"left\", \"right\"]\n",
    "\n",
    "\n",
    "def eval_valuefunc(state, value_function, rewards, gamma=0.9):\n",
    "    x, y = state\n",
    "    new_value = max(\n",
    "        rewards[x, y] + gamma * value_function[move((x, y), action)]\n",
    "        for action in actions\n",
    "    )\n",
    "    return new_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "a1f0ceee-7bd0-48a9-b244-6d6851150010",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_improv(value_func, policy_prev, rewards, gamma, grid_size):\n",
    "    policy_stable=False\n",
    "    while not policy_stable:\n",
    "        policy_stable = True\n",
    "        new_policy = [[\"\" for _ in range(grid_size)] for _ in range(grid_size)]\n",
    "        \n",
    "        for x in range(grid_size):\n",
    "            for y in range(grid_size):\n",
    "                old_action = policy_prev[x][y]\n",
    "                # Find the action that maximizes the value function\n",
    "                action_values = {}\n",
    "                for action in actions:\n",
    "                    new_state = move((x, y), action)\n",
    "                    action_values[action] = rewards[x, y] + gamma * value_func[new_state]\n",
    "\n",
    "                # Get the best action\n",
    "                best_action = max(action_values, key=action_values.get)\n",
    "                new_policy[x][y] = best_action\n",
    "\n",
    "                if old_action != best_action:\n",
    "                    policy_stable = False\n",
    "        \n",
    "        policy_prev = new_policy\n",
    "    \n",
    "    return policy_prev\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "9876a963-ff16-40b6-8bfa-3af8da4feedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_eval(value_func,policy,rewards,gamma=0.9,theta=1e-5,grid_size=4): #find v*pi using bellman\n",
    "    \n",
    "    while True:\n",
    "        delta=0\n",
    "        for x in range(grid_size):\n",
    "            for y in range(grid_size):\n",
    "                v=value_func[x,y]\n",
    "                action=policy[x][y]\n",
    "                new_state=move((x,y),action)\n",
    "                value_func[x, y] = rewards[x, y] + gamma * value_func[new_state]\n",
    "                delta = max(delta, abs(v - value_func[x, y]))\n",
    "        if delta < theta:\n",
    "            break\n",
    "    return value_func\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48da0b0c-fdca-4b94-bd4c-3aaab083e6ee",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "448cf60c-e8e9-4b78-835b-e2385f06bbba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iter(rewards, gamma=0.9, theta=1e-5, grid_size=4):\n",
    "    # Initialize random policy\n",
    "    actions = [\"up\", \"down\", \"left\", \"right\"]\n",
    "    policy = [[np.random.choice(actions) for _ in range(grid_size)] for _ in range(grid_size)]\n",
    "    value_func = np.zeros((grid_size, grid_size))\n",
    "\n",
    "    while True:\n",
    "        # Evaluate the current policy\n",
    "        value_func = policy_eval(value_func, policy, rewards, gamma, theta, grid_size)\n",
    "        \n",
    "        # Improve the policy\n",
    "        new_policy = policy_improv(value_func, policy, rewards, gamma, grid_size)\n",
    "        \n",
    "        # Check if the policy is stable\n",
    "        if new_policy == policy:\n",
    "            break\n",
    "        policy = new_policy\n",
    "\n",
    "    return policy, value_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "e9f1d592-5723-4587-9c93-b5ef2b391bf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Policy:\n",
      "['right', 'right', 'right', 'up']\n",
      "['up', 'up', 'up', 'up']\n",
      "['up', 'right', 'up', 'up']\n",
      "['up', 'up', 'up', 'down']\n",
      "\n",
      "Optimal Value Function:\n",
      "[[ 1.14333333e+01  1.63333333e+01  2.33333333e+01  3.33333333e+01]\n",
      " [ 8.00333333e+00  1.04333333e+01  1.63333333e+01  2.33333333e+01]\n",
      " [ 5.60233333e+00  8.00333333e+00  1.14333333e+01 -2.86666667e+01]\n",
      " [ 4.92163333e+00  5.60233333e+00 -1.19966667e+01 -5.34663125e-06]]\n"
     ]
    }
   ],
   "source": [
    "# Perform policy iteration\n",
    "optimal_policy, optimal_value_func = policy_iter(rewards, gamma, theta, grid_size)\n",
    "\n",
    "# Print the optimal policy and value function\n",
    "print(\"Optimal Policy:\")\n",
    "for row in optimal_policy:\n",
    "    print(row)\n",
    "\n",
    "print(\"\\nOptimal Value Function:\")\n",
    "print(optimal_value_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "45221312-df7d-40d7-a082-b6497ab6c9ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.,   0.,   0.,  10.],\n",
       "       [  0.,  -1.,   0.,   0.],\n",
       "       [  0.,   0.,   0., -45.],\n",
       "       [  1.,   0., -20.,   0.]])"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069881a5-05bd-4c57-bcb1-593969691bec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67d563a-f939-4c3e-8315-d5350f0d64c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232afbe6-e13e-4b52-8d78-4092edc324a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37bc0d0-793e-4e96-9a8c-09772c622bf2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468edc16-54af-407b-87eb-bb035a55b448",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d197c9d-6cf2-4f49-8107-eae4a6ccbe2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23f90cd-0f58-4ee6-b269-a41cb6034989",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
