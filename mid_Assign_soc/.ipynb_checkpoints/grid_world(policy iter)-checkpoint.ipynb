{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a70758a0-a7c5-4df1-9b79-9736504d1e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random as rd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "0496fed7-cd1a-4282-afc7-ab5edd700e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "grid_size=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "17119234-9bc1-4cad-ba4c-e9043b580032",
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_pots={\n",
    "    (0,3): 10,\n",
    "    (1,1): \"0\",\n",
    "    (3,0): 1,\n",
    "    (3,3): 5\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "2923ed24-4e59-4af7-9614-fa9b5a29a7ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0., 10.],\n",
       "       [ 0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.,  5.]])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards=np.zeros((grid_size,grid_size))\n",
    "for pos,reward in reward_pots.items():\n",
    "    rewards[pos]=reward\n",
    "rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "bf650c54-8db2-46f2-a0f9-82d039ae94a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reward(position):\n",
    "    return rewards[position]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f221fe3d-f15b-4260-82f7-598d4d5a4eb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.0"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_reward((0,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "9d1540c2-db78-400a-bb78-99ead8bf2a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the deterministic transition dynamics\n",
    "def move(position, action):\n",
    "    x, y = position\n",
    "    if action == \"up\" and x > 0:\n",
    "        return (x - 1, y)\n",
    "    elif action == \"down\" and x < grid_size - 1:\n",
    "        return (x + 1, y)\n",
    "    elif action == \"left\" and y > 0:\n",
    "        return (x, y - 1)\n",
    "    elif action == \"right\" and y < grid_size - 1:\n",
    "        return (x, y + 1)\n",
    "    else:\n",
    "        return position  # If the move is not possible, stay in the same position\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "413d5459-fc78-4ff9-8cd1-886996a752bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "value_func=np.zeros((4,4))\n",
    "\n",
    "# Value iteration parameters\n",
    "gamma = 0.9  # Discount factor\n",
    "theta = 0.0001  # Threshold for convergence\n",
    "actions = [\"up\", \"down\", \"left\", \"right\"]\n",
    "def eval_valuefunc(state, value_function, rewards, gamma=0.9):\n",
    "    x, y = state\n",
    "    new_value = max(\n",
    "        rewards[x, y] + gamma * value_function[move((x, y), action)]\n",
    "        for action in actions\n",
    "    )\n",
    "    return new_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "a1f0ceee-7bd0-48a9-b244-6d6851150010",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_improv(value_func,policy_prev):\n",
    "    policy_stable=False\n",
    "    while not policy_stable:\n",
    "        policy_stable = True\n",
    "        new_policy = [[\"\" for _ in range(grid_size)] for _ in range(grid_size)]\n",
    "        \n",
    "        for x in range(grid_size):\n",
    "            for y in range(grid_size):\n",
    "                old_action = policy_prev[x][y]\n",
    "                # Find the action that maximizes the value function\n",
    "                action_values = {}\n",
    "                for action in actions:\n",
    "                    new_state = move((x, y), action)\n",
    "                    action_values[action] = rewards[x, y] + gamma * value_func[new_state]\n",
    "\n",
    "                # Get the best action\n",
    "                best_action = max(action_values, key=action_values.get)\n",
    "                new_policy[x][y] = best_action\n",
    "\n",
    "                if old_action != best_action:\n",
    "                    policy_stable = False\n",
    "        \n",
    "        policy_prev = new_policy\n",
    "    \n",
    "    return policy_prev\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9876a963-ff16-40b6-8bfa-3af8da4feedd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc61403b-7edd-485f-97b2-8ad666736958",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
