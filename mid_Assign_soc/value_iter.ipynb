{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0dea6a9-ffac-45fb-93e6-4f2e250f7011",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b7d66c75-d3fc-4b0b-a464-f7c75458186e",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_size=4\n",
    "empty_value=np.zeros((4,4))\n",
    "reward_pots={\n",
    "    (0,3): 10,\n",
    "    (1,1): \"-1\",\n",
    "    (3,0): 1,\n",
    "    (3,2): \"-20\",\n",
    "    (2,3): -45\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "cb17e22d-803f-440e-9faa-cedd0bbc607d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.,   0.,   0.,  10.],\n",
       "       [  0.,  -1.,   0.,   0.],\n",
       "       [  0.,   0.,   0., -45.],\n",
       "       [  1.,   0., -20.,   0.]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards=np.zeros((grid_size,grid_size))\n",
    "for pos,reward in reward_pots.items():\n",
    "    rewards[pos]=reward\n",
    "rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d0f5149b-ce35-42bb-9fe7-69472e537b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the deterministic transition dynamics\n",
    "def move(position, action):\n",
    "    x, y = position\n",
    "    if action == \"U\" and x > 0:\n",
    "        return (x - 1, y)\n",
    "    elif action == \"D\" and x < grid_size - 1:\n",
    "        return (x + 1, y)\n",
    "    elif action == \"L\" and y > 0:\n",
    "        return (x, y - 1)\n",
    "    elif action == \"R\" and y < grid_size - 1:\n",
    "        return (x, y + 1)\n",
    "    else:\n",
    "        return position  # If the move is not possible, stay in the same position\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "495dc1fe-a790-44b6-abad-f9ead4c01605",
   "metadata": {},
   "outputs": [],
   "source": [
    "value_func=np.zeros((4,4))\n",
    "\n",
    "# Value iteration parameters\n",
    "gamma = 0.7  # Discount factor\n",
    "theta = 0.00001  # Threshold for convergence\n",
    "actions = [\"U\", \"D\", \"L\", \"R\"]\n",
    "    \n",
    "\n",
    "def eval_valuefunc(state, value_function, rewards, gamma=0.9):\n",
    "    x, y = state\n",
    "    actions = [\"U\", \"D\", \"L\", \"R\"]\n",
    "    \n",
    "    new_value = max(\n",
    "        rewards[x, y] + gamma * value_function[move((x, y), action)]\n",
    "        for action in actions\n",
    "    )\n",
    "    return new_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f65cd01b-c3f9-4c0e-b16b-eb78ddafce90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iter(value_func,rewards,gamma=0.9,theta=1e-5):\n",
    "    \n",
    "    while True:\n",
    "        delta=0\n",
    "        new_value_func = np.zeros((grid_size, grid_size))\n",
    "        \n",
    "        for x in range(grid_size):\n",
    "            for y in range(grid_size):\n",
    "                state=(x,y)\n",
    "                \n",
    "                v = value_func[x, y]\n",
    "                new_value_func[x,y]=eval_valuefunc(state, value_func, rewards, gamma=0.9)\n",
    "                delta = max(delta, abs(v - new_value_func[x, y]))\n",
    "        value_func = new_value_func.copy()\n",
    "        \n",
    "        if delta < theta:\n",
    "            break\n",
    "    \n",
    "    return value_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1d6b7947-0044-4d10-9dae-60d489d98e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_optimal_policy(value_func, rewards, gamma=0.9):\n",
    "    grid_size = len(value_func)\n",
    "    actions = [\"U\", \"D\", \"L\", \"R\"]\n",
    "    policy = np.empty((grid_size, grid_size), dtype=str)\n",
    "    \n",
    "    for x in range(grid_size):\n",
    "        for y in range(grid_size):\n",
    "            state = (x, y)\n",
    "            print(state)\n",
    "            action_values = {}\n",
    "            \n",
    "            for action in actions:\n",
    "                new_state = move(state, action)\n",
    "                \n",
    "                action_values[action] = rewards[x, y] + gamma * value_func[new_state]\n",
    "                print(f'action:{action} and action_value: {action_values[action]}')\n",
    "            \n",
    "            best_action = max(action_values, key=action_values.get)\n",
    "            print(best_action)\n",
    "            policy[x, y] = best_action\n",
    "    \n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b4a3efd4-f878-4786-959c-e9d3294e6756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0)\n",
      "action:U and action_value: 51.029942541829264\n",
      "action:D and action_value: 45.92694254182927\n",
      "action:L and action_value: 51.029942541829264\n",
      "action:R and action_value: 56.69994254182926\n",
      "R\n",
      "(0, 1)\n",
      "action:U and action_value: 56.69994254182926\n",
      "action:D and action_value: 50.32994254182927\n",
      "action:L and action_value: 51.029942541829264\n",
      "action:R and action_value: 62.99994254182926\n",
      "R\n",
      "(0, 2)\n",
      "action:U and action_value: 62.99994254182926\n",
      "action:D and action_value: 56.69994254182926\n",
      "action:L and action_value: 56.69994254182926\n",
      "action:R and action_value: 69.99994254182926\n",
      "R\n",
      "(0, 3)\n",
      "action:U and action_value: 79.99994254182926\n",
      "action:D and action_value: 72.99994254182926\n",
      "action:L and action_value: 72.99994254182926\n",
      "action:R and action_value: 79.99994254182926\n",
      "U\n",
      "(1, 0)\n",
      "action:U and action_value: 51.029942541829264\n",
      "action:D and action_value: 41.334242541829276\n",
      "action:L and action_value: 45.92694254182927\n",
      "action:R and action_value: 50.32994254182927\n",
      "U\n",
      "(1, 1)\n",
      "action:U and action_value: 55.69994254182926\n",
      "action:D and action_value: 44.92694254182927\n",
      "action:L and action_value: 44.92694254182927\n",
      "action:R and action_value: 55.69994254182926\n",
      "U\n",
      "(1, 2)\n",
      "action:U and action_value: 62.99994254182926\n",
      "action:D and action_value: 51.029942541829264\n",
      "action:L and action_value: 50.32994254182927\n",
      "action:R and action_value: 62.99994254182926\n",
      "U\n",
      "(1, 3)\n",
      "action:U and action_value: 69.99994254182926\n",
      "action:D and action_value: 25.199942541829266\n",
      "action:L and action_value: 56.69994254182926\n",
      "action:R and action_value: 62.99994254182926\n",
      "U\n",
      "(2, 0)\n",
      "action:U and action_value: 45.92694254182927\n",
      "action:D and action_value: 37.90081254182928\n",
      "action:L and action_value: 41.334242541829276\n",
      "action:R and action_value: 45.92694254182927\n",
      "U\n",
      "(2, 1)\n",
      "action:U and action_value: 50.32994254182927\n",
      "action:D and action_value: 41.334242541829276\n",
      "action:L and action_value: 41.334242541829276\n",
      "action:R and action_value: 51.029942541829264\n",
      "R\n",
      "(2, 2)\n",
      "action:U and action_value: 56.69994254182926\n",
      "action:D and action_value: 31.926942541829273\n",
      "action:L and action_value: 45.92694254182927\n",
      "action:R and action_value: 25.199942541829266\n",
      "U\n",
      "(2, 3)\n",
      "action:U and action_value: 17.999942541829263\n",
      "action:D and action_value: -16.265757458170725\n",
      "action:L and action_value: 6.029942541829264\n",
      "action:R and action_value: -19.800057458170734\n",
      "U\n",
      "(3, 0)\n",
      "action:U and action_value: 42.334242541829276\n",
      "action:D and action_value: 38.90081254182928\n",
      "action:L and action_value: 38.90081254182928\n",
      "action:R and action_value: 42.334242541829276\n",
      "U\n",
      "(3, 1)\n",
      "action:U and action_value: 45.92694254182927\n",
      "action:D and action_value: 41.334242541829276\n",
      "action:L and action_value: 37.90081254182928\n",
      "action:R and action_value: 31.926942541829273\n",
      "U\n",
      "(3, 2)\n",
      "action:U and action_value: 31.029942541829264\n",
      "action:D and action_value: 11.926942541829273\n",
      "action:L and action_value: 21.334242541829276\n",
      "action:R and action_value: 8.734242541829275\n",
      "U\n",
      "(3, 3)\n",
      "action:U and action_value: 25.199942541829266\n",
      "action:D and action_value: 28.734242541829275\n",
      "action:L and action_value: 31.926942541829273\n",
      "action:R and action_value: 28.734242541829275\n",
      "L\n",
      "Optimal Value Function:\n",
      "[[  0.   0.   0.  10.]\n",
      " [  0.  -1.   0.   0.]\n",
      " [  0.   0.   0. -45.]\n",
      " [  1.   0. -20.   0.]]\n",
      "\n",
      "Optimal Policy:\n",
      "R R R U\n",
      "U U U U\n",
      "U R U U\n",
      "U U U L\n",
      "[[72.89991792 80.99991792 89.99991792 99.99991792]\n",
      " [65.60991792 71.89991792 80.99991792 89.99991792]\n",
      " [59.04891792 65.60991792 72.89991792 35.99991792]\n",
      " [54.14401792 59.04891792 45.60991792 41.04891792]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "optimal_value_func = value_iter(empty_value, rewards, gamma, theta)\n",
    "optimal_policy = find_optimal_policy(optimal_value_func, rewards, gamma)\n",
    "\n",
    "# Print the optimal value function\n",
    "print(\"Optimal Value Function:\")\n",
    "print(rewards)\n",
    "print(\"\\nOptimal Policy:\")\n",
    "for row in optimal_policy:\n",
    "    print(\" \".join(row))\n",
    "print(optimal_value_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27c3dbc-06fe-40fb-9984-481615ec3ccc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4e250d-4160-4780-bf52-370599ee3c03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
